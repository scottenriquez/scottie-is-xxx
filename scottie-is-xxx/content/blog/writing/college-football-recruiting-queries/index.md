---
title: "Creating College Football Recruiting Database on AWS Athena"
date: "2020-06-09T22:12:03.284Z"
description: "A set of tools to fetch and process publicly available data from 247 for non-commercial, personal data analysis use."
---

## About College Football Recruiting 
Despite being played by amateur student-athletes, college football has become a multi-billion dollar industry. Most likely due to the emotional connection to an academic institution and the incredibly volatile lack of parity and consistency amongst teams, college football fans tend to be even more diehard than their NFL counterparts, particularly in the South. Though college football is played by undergraduate and graduate students, players are scouted as recruits as early as middle school. These recruits are evaluated based on several factors that indicate their success at both the collegiate and professional levels of football. Whether physical attributes like height and weight or skill sets like blocking and catching, all of these attributes plus countless others are synthesized into a rating. Recruits are then offered by universities culminating in commitments and signings. Having a good recruiting class can be an indication of future success for your team provided that the coaching staff develops talents as expected.

## Source Code
This [repository](https://github.com/scottenriquez/247-recruiting-ranking-history-scraper) is a set of Python and shell scripts to fetch and process publicly available data from 247 for non-commercial, personal data analysis use to be done using AWS Athena. It's co-authored by [Callen Trail](https://callen.xyz). Please note that the code is brittle. If any of the HTML changes on the site, the scripts will need to be modified.

## Stage One: Fetching Recruit Lists by Year
Players are scraped from the recruiting index page in the following format:
```json
{
    "247_id": "46038819",
    "247_url": "https://247sports.com/Player/Bryan-Bresee-46038819",
    "full_name": "Bryan Bresee",
    "year": 2020,
    "position": "DT",
    "high_school": "Damascus",
    "city": "Damascus",
    "state": "MD",
    "score": "0.9995",
    "stars": 5,
    "height_feet": 6.0,
    "height_inches": 5.0,
    "weight": 290.0
}
``` 
There's also some basic exception handling to insert default values for inconsistent or missing data.
![Error handling](recruit-list-error-handling.png)

Before running the initial script, be sure to change the year range to fetch for in `scrape_recruit_list.py`:
```python
year_range = range(2010, 2021)
```

All dependencies are located in `requirements.txt`. To run, simple execute the command `python scrape_recruit_list.py`. The script will generate a file for each year (i.e. `recruit-list-2020.json`) in the `/recruit-lists` directory. These directories are treated as output and ignored via the `.gitignore`.

## Stage Two: Obtaining Ranking History and Recruiting Timeline Events
With a set of lists generated by stage one, the `process_recruits.py` script fetches and parses the complete ranking history and events (i.e. official visits, offers, etc.). To run, pass a recruiting list from stage one and the corresponding year to produce the files: `python process_recruits.py 2020 recruit-lists/recruit-list-2020.json`. 247's data isn't perfectly formatted, so stage three fixes issues from stage two and formats for more efficient querying using AWS Athena.
![Error handling](process-recruits-error-handling.png)

Recruit ranking histories are stored in the following path: `/recruit-ranking-histories/{year}/{247_id}.json`. For example, Bryan Bresee's path would be `/recruit-ranking-histories/2020/46038819.json` in the following format:
```json
{
    "247_id": "46038819",
    "rating": 0.9995,
    "rank": 1,
    "change_date": "2020-01-31",
    "delta": -0.0002,
    "delta_inception_value": 0.0295
}
```

Recruiting timeline events are stored in the following path: `/recruit-timeline-histories/{year}/{247_id}.json`. For example, Bryan Bresee's path would be `/recruit-timeline-histories/2020/46038819.json` in the following format:
```json
{
    "247_id": "46038819",
    "event_date": "2020-01-08",
    "event_type": "Enrollment",
    "event_description": "Bryan Bresee enrolls at Clemson Tigers"
}
```

Given the large amount of data to process during stage two, this project also includes a bootstrapping script for EC2 instances to install the Python tooling, configure the virtual environment, and pull the data from stage one via S3:
```shell script
#!/bin/bash
sudo yum install git -y
sudo yum install python3 -y
git clone https://github.com/scottenriquez/247-recruiting-ranking-history-scraper.git
cd 247-recruiting-ranking-history-scraper
mkdir recruit-lists
mkdir recruit-ranking-histories
mkdir recruit-timeline-histories
aws s3 cp s3://247-recruit-rankings-2010-2020/recruit-list/ recruit-lists --recursive
python3 -m venv env
source env/bin/activate
sudo pip3 install -r requirements.txt
```

Note that since S3 bucket names are globally unique, this will need to be changed for any other bucket.

## Stage Three: Cleanup, Normalization, and Optimization
After the first two stages, there should be three directories of data:
- `/recruit-lists` contains one file per year containing all recruits from that year
- `/recruit-ranking-histories` contains subdirectories for each year storing an individual JSON file per recruit capturing ranking changes
- `/recruit-timeline-histories` contains subdirectories for each year storing an individual JSON file per recruit capturing events like official visits

Several scrips to format this raw data are maintained in this repository. The first of these is `merge_utility.py` which merges all files in each of the top level directories into unified lists. This can be easier to manage than handling the thousands of files generated by the application depending on use case. Specifically it is more peformant for AWS Athena which prefers larger files as opposed to a higher volume of files. To run, use the command `python merge_utility.py <PATH_TO_DIRECTORY_WITH_FILES_TO_MERGE> <PATH_TO_OUTPUT_FILE>`.

Numerous duplicate recruits exist after producing the recruit lists in stage one, so `duplicate_utility.py` can be run to clean a stage one file in place: `python duplicate_utility.py <PATH_TO_RECRUIT_LIST_FILE>`.

## Configuring AWS Athena
For this project, Athena is cheaper and simpler to stand up than using a proper relational database which would require additional ETL jobs to migrate from the JSON source files to the tables. Athena uses serverless compute to query these raw files directly from S3 with ANSI SQL. After Athena and the Glue Data Catalog have been configured, SQL queries can be run against the datasets in S3 buckets. For example, this query computes when commits from the 2020 class were extended offers by the University of Texas at Austin:
```sql
select recruit.full_name, timeline.event_type, timeline.event_date, timeline.event_description
from timeline_events timeline
join recruit_list recruit on  recruit."247_id" = timeline."247_id"
where timeline.event_type = 'Offer' and timeline.event_description like '%Texas Longhorns%' and recruit.year = 2020
order by event_date desc
```
